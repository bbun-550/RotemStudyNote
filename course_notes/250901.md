250901

## 분류 분석

#### decision tree
>실습파일
>[[deci01.py]] : 의사결정나무
>[[deci02_iris.py]] : graphbiz 시각화
>[[deci03_over.py]] : 과적합 방지 처리 방법
>[[deci04_ensemble.py]]

>참고 : 
>통계기초 p86 의사결정나무
>분류나무 모형은 불연속적(이산형 자료)인 값을 예측한다. 예를 들어 분류 모델은 다음과 같은 질문에 대한 답을 예측한다. ‘수신된 이메일이 스팸인가 아닌가?’, ‘이 사진이 강아지인가 고양이인가 또는 햄스터인가?’와 같은 질문에 답을 내기 적합한 비지도 분류 알고리즘 모형이다
>[결정트리, 균일한 데이터, 결정트리 파라미터](https://jaaamj.tistory.com/21)
> 불순도 Impurity / 
>- 불순도가 높으면 클래스가 많다. 모델의 성능이 떨어진다.
>-eg. 10명의 학생을 합불로 분류 => 합 8명, 불 2명
>- 지니계수 $1-(0.8^2+0.2^2)$ 이 값이 0에 가까울수록 불순도가 낮다.

- 정량적/정성적 작업 모두 할 수 있음
- 수치형/범주형 종속 변수에 대한 예측/분류 사용한다.

- 결정 트리에서 질문이나 정답을 담은 네모 상자를 노드(Node)라고 한다.

>정보의 균일도를 기준으로 정보를 분류한다.
>- feature(독립변수) ; 하나 또는 두 개
>	- 혼잡도가 높다(엔트로피) => 결정트리는 혼잡도를 최소화하는게 목표이다.
>	- 끊임없이 가지치기를 한다. impurity가 0일 될 때까지
>	- 즉, 이진분류를 불순도가 0일 될 때까지 진행한다.
> $\text{Entropy}=-\sum(p_{i})log_{2}{p_{i}}$
>- 지니계수 : 소득 분배의 불평등 정도를 나타내는 대표적인 지표로, 0과 1 사이의 값을 가진다. 

- random forest로 연결된다.

아래 방법은 원본 데이터로 해보고 과적합 나오면 하나씩 진행해보는 것이다.

#### train_test_split

#### K-fold
- validation : 학습 도중에 test를 할 수 있다.
	- 5 fold일 경우, 5번 접어서 
	- 5번의 평균을 낸다. - 교차검정

#### GridSearchCV
- 최적의 파라미터를 찾아주는 방법.
	- 파마미터 후보값들을 사전에 정의해주고 이를 일일이 for문 처럼 돌려서 적용해보는 것을 대신해준다.
- 다양한 조합에 대해서 검증하고 best 파라미터를 찾아준다. 
- 결과물로 max_depth, min_sample_split 값을 알려준다.


#### 앙상블 ensemble >> 면접 많이 물어봄

- Voting : 샘플링 데이터 그대로 사용
	- hard voting : 예측한 결괏값들중 다수의 분류기가 결정한 예측값을 최종 보팅 결괏값으로 선정하는 방식이다. (다수결 원칙)
	- soft voting : 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결괏값으로 선정한다
	- 각 분류기의 레이블 값 예측 확률을 평균 내어 최종 결정을 한다

- Bagging : 병렬
- Boosting : 샘플링 데이터 일부만 바꿔서 학습, sequencial

- 앙상블 장단점
	- 특정 훈련 데이터 세트에서만 정확도를 보장하는 과대적합(Overfitting)현상을어느 정도 줄여 줄 수 있다


#### Random forest
- 의사결정나무 여러개를 묶어서 사용한다(기본 500개, 실무에서 2000개)


#### KNN

