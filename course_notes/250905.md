250905
>실습파일
>knn01.py : breast_cancer 데이터로 knn 실습 - KNeighborsClassifier()
>knn02_iris.py :  복붙 파일
>ann01.py : 단층 신경망
>ann02.py : 단층 신경망 - 퍼셉트론 예제
>ann03_iris.py : 단층 신경망 퍼셉트론 모델 iris 시각화
>ann04_iris.py : iris 3d 시각화
>#### 오후
>mlp01.py : 다층 신경망 - MLPClassifier
>mlp02.py : MLP 실습 - 종양데이터
>mlp03_iris.py : MLPClassifier iris 시각화

python_knn01
tag : deeplearning
## 최근접 이웃 K Nearest Neighbors
- 임의 데이터와 가장 가까운 거리의 데이터를 찾아서 다수결로 데이터를 예측하는 방법이다.

- 예측하고 싶은 데이터가 있으면, 그 주변에 있는 k 개의 이웃을 보고 분류하는 방법이다.
	1. 새 데이터와 기존 데이터 간의 거리를 계산한다.
	2. 가장 가까운 k개 이웃을 선택한다.(이때 k는 지정할 수 있다. k를 얼마로 지정하느냐에 따라 결과값이 달라지므로 중요하다.)
	3. k개 이웃들 중 가장 많은 그룹을 새 데이터의 예측 결과로 선택한다.

- 분류할 때 새데이터와 기존 데이터 간의 거리를 계산한다. 이때 두 가지 방법이 있다.
	- 맨해튼 거리 계산법
		- 격자형 길을 따라서 최적의 루트를 계산하는 방법이다.
	- 유클리드 거리 계산법(가장 많이 사용한다.)
		- 대각선으로 최적의 루트를 계산하는 방법이다.

#### 실습 - breast_cancer 데이터(+시각화)
- 활용 라이브러리
```python
from sklearn.datasets import load_breast_cancer # 데이터 호출
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier 
import matplotlib.pyplot as plt
```
- `KNeighborsClassifier()` 
	- `n_neighbors=`: 이웃 수(기본 5). 작으면 유연하지만 노이즈 민감 과대적합, 크면 과소적합될 수 있다. 그래서 보통 홀수로 설정한다.
	- `p`: Minkowski 거리의 차수(기본 2). p=1은 Manhattan, p=2는 Euclidean. metric이 'minkowski'일 때만 의미 있다.
	- `metric=`: 거리 함수(기본 'minkowski'). 'euclidean', 'manhattan', 'chebyshev', 'wminkowski', 'seuclidean', 'mahalanobis' 등 또는 callable. 비표준 metric이면 알고리즘이 'brute'로 강제될 수 있다.

	- `metric_params=`: 거리 함수 추가 파라미터. 예) 'mahalanobis'는 {'VI': 공분산역행렬} 또는 {'V': 공분산행렬}, 'seuclidean'은 {'V': 특성별 분산}, 'wminkowski'는 {'w': 특성가중}.
	- `n_jobs=`: 병렬 처리 코어 수(기본 None). -1은 모든 코어 사용. 데이터가 크거나 이웃 수가 클 때 속도 개선한다.
	- `weights=`: 이웃 가중 방식. 'uniform'(동일 가중, 기본), 'distance'(가까울수록 큰 가중), 또는 callable(distances -> weights). 밀도 차이 크거나 경계 구분이 모호하면 'distance'가 유리.
	- `algorithm=`: 이웃 탐색 알고리즘. 'auto'(기본), 'ball_tree', 'kd_tree', 'brute'. 고차원(>20~30)이나 특이 metric이면 'brute'가 종종 더 빠르다.
	- `leaf_size=`: 트리 리프 크기(기본 30). KD/Ball Tree의 속도·메모리 트레이드오프에 영향. 대체로 기본 유지, 대용량에서만 튜닝.

---

## 인공신경망 Artificial Neural Network
- 기계학습과 인지과학 분야에서 고안한 학습 알고리즘이다.

- 신경세포 : 가지돌기, 신경세포, 축삭돌기 ... 

#### 퍼셉트론 - 인공신경망의 시작
- 인경신경망 안에 가장 단순한 기본 단위 모델이다.


- 외부의 데이터가 들어오면 가중치(w, 기울기)를 준다
	- 각 변수의 최적의 w를 찾는다. x값을 잘 설명할 수 있는 w를 구하는 과정이다. 
- 가중치 작업된 데이터를 인공 뉴런이 합한 값으로 받는다.(여기서 내적을 쓴다. 입력 값이 벡터이기 때문이다.)
- 활성화 함수를 거쳐서 y값을 출력한다.
	- cost, error, 손실 = 예측값 - 실제값 (둘 차이가 작아야 제대로 예측된 것이다. 차이가 크다면 제대로된 w, b를 찾지 못한 것이기 때문에 다시 학습시킨다.)
	- w 값을 조정할 때 미분의 개념을 이해해야 한다.
	- [미분 개념 읽을 자료](https://cafe.daum.net/flowlife/S2Ul/64?svc=toprank)
>[!INFO] 미분 / 편미분
> 편미분 : 다변수 함수, 여러 변수를 모두 미분하는데 임의의 한 변수를 편미분할 때 다른 변수는 변하지 않는다고 계산하는 방법이다. 결과 값은 벡터로 나온다. eg. $x^2+y^2$를 편미분하면 
>$\nablaf=(\partialx\partialf​,\partialy\partialf​)=(2x,2y)$
>결론 : 미분계수는 평균변화율의 극한인데 기하학적으로 보면 그 점에서 접하는 기울기가 바로 미분계수다. 기울기는 곧 순간 변화율을 말한다.
>가정: 서울 -> 부산 (400km, 4시간)
>속력 = 거리 변화량(y축) /시간 변화량(x축) => 기울기, 평균변화율 ; 두 개의 점이 전제되어야 한다.
>- 대전의 특정 지점을 지나갈 때, 순간 속력은 얼마인가?
>- 미분 계수 : 평균 변화율의 극한이다

>[!SUMMARY] 내가 이해한 바
> 퍼셉트론에서는 입력값의 가중치를 랜덤하게 준다. 그러면 도출된 예측값과 실제값의 차이가 큰값이 도출될 수 있기 때문에 다시 학습을 반복한다. 다시 학습을 반복할 때 w 가중치를 조정한다. w 가중치를 조정할 때 편미분으로 조정한다. 여기서 w 가중치를 조정하는 방법을 경사하강법이라고 부른다.

- 단층 퍼셉트론은 XOR를 해결할 수 없다. 해결하기 위해서는 노드(인공뉴런)을 추가해야 한다.
	- 단층 퍼셉트론으로 AND, OR, NAND(=NOT AND)와 같은 비교적 단순한 논리 게이트는 구현할
수 있었지만 XOR같은 문제를 풀 수 없는 한계가 있었다.
	- 단층 퍼셉트론은 선형분류기이다. 하지만 xor은 직선으로 분류되지 않기 떄문이다.

- $w_T\cdotx + b = \sum_{i=d}^{d} w_{i}x_{i} + b$ : $w_{i}x_{i}$는 스칼라 곱이고 그 합이 내적이다.

- 다층 퍼셉트론 등장!

- [ML 관련 읽을 거리](https://cafe.daum.net/flowlife/SBU0/20?svc=toprank)



#### 손실함수 Loss Function


---

[Latex 문법 정리](https://en.wikibooks.org/wiki/LaTeX/Mathematics)

