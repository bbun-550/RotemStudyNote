250826


- ols
	- 장점은 summary()


>실습파일
>[[lm09_carseats.py]] : 어제 내용 이어서 (선형회귀분석의 기존 가정 충족 조건)
4.등분산성 ~ 5.다중공산성
>2교시 [[lm09_etc.py]] : joblib로 만든 모델 불러오기
>[[lm10_lr.py]] : linear regression - sklearn 모듈의 linearRegression 클래스 사용, 모델 성능 평가
>[[lm11.py]] : 선형회귀 평가 지표 관련
>[[lm12.py]] : linearRegression - mtcars
>[[lm13_penalty.py]] : 

- 모델을 생성(pickle 또는 joblib로 저장)하면 위에 과정을 반복할 필요없다.

#### 등분산성

#### 다중공선성
- 종속변수 1개, 독립변수 여러 개
	- y ~ x1 성적, x2 나이, x3 학년
	- 독립변수 간에 상관관계가 있으면 안된다. 상관관계가 낮아야 한다.
	- 나이 ~ 학년 간 상관관계가 높을 것이 의심된다.
	- 조치 : 둘 중 하나를 제거 또는 차원축소(PCA ; 주성분 분석? ...)
- 다중공선성이 걸려있으면, 같은 변수를 두 번 넣은것과 같다. 

- 확인 방법(여러개)
	- VIF : VIF 값이 10보다 크면 문제가 있다.
		- 더미변수일 경우에는 3보다 크면 다중공선성 의심한다.
	- 상태지수

- 선형회귀분석 하기 전에 <선형회귀분석의 기존 가정 충족 조건>을 먼저 만족해야 한다.

## LinearRegression 클래스 사용
- `from sklearn.linear_model import LinearRegression`
- `from sklearn.metrics import r2_score` : 결정계수를 계산하는 함수
- `explained_variance_score` : 설명된 분산 비율 계산
	- 예측값과 실제값의 분산 차이가 적을수록 값이 1에 가깝다
	- r2_score와 비슷하지만, 평균편향을 고려하지 않았다.
	- 여러 모델 비교할 때 유용하다.
- `mean_absolute_error` : 평균 절대 오차 계산
	- 예측값과 실제값의 평균적인 절대 오차 크기
	- 값이 작을수록 예측 정확도가 높다.
	- 얼마나 틀렸는지 직관적으로 알려준다.(단위 그대로 해석 가능하다.)

#### 선형회귀의 평가 지표
- MAE

- MSE

- RMSE

- $R^2$ 결정계수

sklearn : 입력 2차원 배열, 출력 1차원으로 되게 만들어져 있다.

#### 선형회귀 평가 지표 관련
- `from sklearn.model_selection import train_test_split`
	- 학습용(train)과 테스트용(test)으로 나누는 함수이다.
- ![[train_test_split.png]]
- 전체 데이터를 일부를 잘라서 학습할 때는 training data로 학습, 평가할 때는 패턴이 비슷한 test data로 평가하면 조금 더 합리적일 수 있다.
- sort해서 자르면 안된다. 그리고 training data랑 test data 패턴은 비슷해야 한다.
- 오버피팅 방지할 수 있다. 

- training 중 training data를 validation 으로 또 잘라서 학습한다.

- 데이터 양이 적을 때는 validation을 안만들기도 한다.

- 주로 8~7 : 2 ~ 3으로 쪼개서 작업을 한다. 

- 모델 성능 평가 결정계수
	- $R^2=\frac{SSR}{SST}$

[카페 - 회귀분석 간단 예제](https://cafe.daum.net/flowlife/SBU0/27?svc=toprank)
- Linear Regression의 기본 알고리즘에 오버피팅 방지 목적의 제약조건을 담은 Ridge, Lasso, ElasticNet 회귀모형이 있다.
- sklearn에 있다.
- 패널티(제약)를 준다.
- linear 대신 Ridge를 쓴다
	- Ridge L2규제 회귀 모형에서는 가중치들의 제곱합(squared sum of weights)을 최소화하는 것을 추가적인 제약 조건으로 한다
	- Ridge: alpha값을 조정(가중치 제곱합을 최소화)하여 과대/과소적합을 피한다. 다중공선성 문제 처리에 효과적.

- Lasso L1규제
	- Lasso: alpha값을 조정(가중치 절대값의 합을 최소화)하여 과대/과소적합을 피한다.
>장단점을 알아야 한다.
>면접 때 질문에 답할 수 있을 정도.
![[l1l2elastic.png]]

- 팁
```python
# 파이프라인: (스케일러 → 모델)
# scikit-learn 에서 여러 전처리기와 모델을 한 번에 묶어서 하나의 모델처럼 쓸 수 있게 해주는 도구
lr = make_pipeline(StandardScaler(with_mean=True), LinearRegression())
ridge  = make_pipeline(StandardScaler(with_mean=True), Ridge(alpha=1.0, random_state=42))
lasso  = make_pipeline(StandardScaler(with_mean=True), Lasso(alpha=0.1, random_state=42, max_iter=10000))
elastic = make_pipeline(StandardScaler(with_mean=True), ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=10000))
```

---
편차, 잔차, 오차의 정의

편차라는 어휘가 사용되고 있는데 편차와 거의 같은 개념으로 사용되는 어휘로 잔차와 오차가 있다. 이들의 의미가 서로 크게 다르지 않아 일반적으로 서로 혼용되는 경향이 있다.

첫째, 편차는 관측치가 평균으로부터의 떨어져 있는 정도, 즉 평균과의 차이를 말한다.

둘째, 잔차는 평균이 아니라 회귀식 등으로 추정된 값과의 차이를 말한다. 즉, 추정된 값이 설명할 수 없어서 아직도 남아있는 편차라는 말로 해석할 수 있다. 따라서 잔차는 편차의 일부분이 된다고도 볼 수 있다.

셋째, 오차는 편차와 달리 예측하기 위하여 추정된 값과 실제값의 차이를 말한다. 이러한 오차는 잔차와 거의 같은 부분을 말하는 경우가 많다. 그러나 잔차는 편차의 일부분이라 생각하고, 오차는 평균을 생각할 필요없이 단순히 예측값 혹은 기대값과 실제값의 차이, 즉 예측값이 정확하지 못한 정도를 나타낸다고 보는 것이 바람직하다.



---
문제해결능력
- 동일한 일을 문제라고 인식
- 해결하기 위해 방법을 고안하고 대화를 한다